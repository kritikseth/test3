{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kritikseth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in '/Users/kritikseth/nltk_data/corpora/stopwords'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kritikseth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import datasets\n",
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer\n",
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from torch.optim import AdamW\n",
    "# from transformers import get_scheduler\n",
    "# import torch\n",
    "# from tqdm.auto import tqdm\n",
    "# import evaluate\n",
    "import random\n",
    "import argparse\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "def example_transform(example):\n",
    "    example[\"text\"] = example[\"text\"].lower()\n",
    "    return example\n",
    "\n",
    "\n",
    "### Rough guidelines --- typos\n",
    "# For typos, you can try to simulate nearest keys on the QWERTY keyboard for some of the letter (e.g. vowels)\n",
    "# You can randomly select each word with some fixed probability, and replace random letters in that word with one of the\n",
    "# nearest keys on the keyboard. You can vary the random probablity or which letters to use to achieve the desired accuracy.\n",
    "\n",
    "\n",
    "### Rough guidelines --- synonym replacement\n",
    "# For synonyms, use can rely on wordnet (already imported here). Wordnet (https://www.nltk.org/howto/wordnet.html) includes\n",
    "# something called synsets (which stands for synonymous words) and for each of them, lemmas() should give you a possible synonym word.\n",
    "# You can randomly select each word with some fixed probability to replace by a synonym.\n",
    "\n",
    "def add_typing_error(sentence):\n",
    "    key_neighbors = {\n",
    "        'a': ['q', 's'], 'b': ['v', 'g'], 'c': ['x', 'd'], 'd': ['s', 'f'],\n",
    "        'e': ['w', 'r'], 'f': ['d', 'g'], 'g': ['f', 'h'], 'h': ['g', 'j'],\n",
    "        'i': ['u', 'o'], 'j': ['h', 'k'], 'k': ['j', 'l'], 'l': ['k', ';'],\n",
    "        'm': ['n', ','], 'n': ['b', 'm'], 'o': ['i', 'p'], 'p': ['o', '['],\n",
    "        'q': ['a', 'w'], 'r': ['e', 't'], 's': ['a', 'd'], 't': ['r', 'y'],\n",
    "        'u': ['y', 'i'], 'v': ['c', 'b'], 'w': ['q', 'e'], 'x': ['z', 'c'],\n",
    "        'y': ['t', 'u'], 'z': ['x', 's']\n",
    "    }\n",
    "\n",
    "    words = sentence.split()\n",
    "    transformed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if random.random() <= 0.3 and len(word) > 2:\n",
    "            for _ in range(random.randint(1, 2)):\n",
    "                index = random.randint(0, len(word) - 1)\n",
    "                original_letter = word[index]\n",
    "                if original_letter.isalpha():\n",
    "                    replacement_letter = random.choice(key_neighbors.get(original_letter, [original_letter]))\n",
    "                    word = word[:index] + replacement_letter + word[index + 1:]\n",
    "\n",
    "        transformed_words.append(word)\n",
    "\n",
    "    return \" \".join(transformed_words)\n",
    "\n",
    "def replace_words_with_synonyms(sentence):\n",
    "    words = sentence.split()\n",
    "    transformed_words = []\n",
    "\n",
    "    num_replacements = random.randint(1, 2)\n",
    "    replacements_made = 0\n",
    "\n",
    "    for word in words:\n",
    "        if replacements_made < num_replacements and word not in stopwords:\n",
    "            if random.random() <= 0.4:\n",
    "                synonyms = get_synonyms(word)\n",
    "                if synonyms:\n",
    "                    transformed_words.append(random.choice(synonyms))\n",
    "                    replacements_made += 1\n",
    "                else:\n",
    "                    transformed_words.append(word)\n",
    "            else:\n",
    "                transformed_words.append(word)\n",
    "        else:\n",
    "            transformed_words.append(word)\n",
    "\n",
    "    return \" \".join(transformed_words)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def replace_preposition(sentence):\n",
    "\n",
    "    prepositions = {\n",
    "        \"at\": [\"in\", \"on\", \"to\", \"for\"],\n",
    "        \"in\": [\"into\", \"at\", \"on\", \"to\",  \"for\"],\n",
    "        \"on\": [\"onto\", \"at\", \"in\", \"to\", \"for\"],\n",
    "        \"to\": [\"at\", \"in\", \"on\", \"for\"],\n",
    "        \"for\": [\"at\", \"in\", \"on\", \"to\"]\n",
    "    }\n",
    "\n",
    "    words = sentence.split()\n",
    "    transformed_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in prepositions:\n",
    "            replacement = random.choice(prepositions[word.lower()])\n",
    "            transformed_words.append(replacement)\n",
    "        else:\n",
    "            transformed_words.append(word)\n",
    "    \n",
    "    return \" \".join(transformed_words)\n",
    "\n",
    "def replace_or_remove_articles(sentence):\n",
    "    articles = [\"a\", \"an\", \"the\"]\n",
    "    words = sentence.split()\n",
    "    transformed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in articles:\n",
    "            if random.random() <= 0.5:\n",
    "                transformed_words.append(\"\")\n",
    "            else:\n",
    "                incorrect_articles = [a for a in articles if a != word.lower()]\n",
    "                replacement = random.choice(incorrect_articles)\n",
    "                transformed_words.append(replacement)\n",
    "        else:\n",
    "            transformed_words.append(word)\n",
    "    \n",
    "    return \" \".join(transformed_words)\n",
    "\n",
    "def remove_be_forms(sentence):\n",
    "    be_forms = [\"am\", \"is\", \"are\", \"was\", \"were\"]\n",
    "    words = sentence.split()\n",
    "    transformed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in be_forms:\n",
    "            transformed_words.append(\"\")\n",
    "        else:\n",
    "            transformed_words.append(word)\n",
    "    \n",
    "    return \" \".join(transformed_words)\n",
    "\n",
    "def custom_transform(example):\n",
    "    ################################\n",
    "    ##### YOUR CODE BEGINGS HERE ###\n",
    "\n",
    "    # Design and implement the transformation as mentioned in pdf\n",
    "    # You are free to implement any transformation but the comments at the top roughly describe\n",
    "    # how you could implement two of them --- synonym replacement and typos.\n",
    "\n",
    "    # You should update example[\"text\"] using your transformation\n",
    "\n",
    "    ###\n",
    "    sentences = example[\"text\"].split(\".\")\n",
    "    transformed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        transformed_sentence = replace_words_with_synonyms(sentence)\n",
    "        transformed_sentence = add_typing_error(transformed_sentence)\n",
    "\n",
    "        if random.random() <= 0.75:\n",
    "            additional_transformations = [replace_preposition, replace_or_remove_articles, remove_be_forms]\n",
    "            selected_transformations = random.sample(additional_transformations, random.randint(1, 3))\n",
    "\n",
    "            for transformation in selected_transformations:\n",
    "                transformed_sentence = transformation(transformed_sentence)\n",
    "\n",
    "        transformed_sentences.append(transformed_sentence)\n",
    "\n",
    "    example[\"text\"] = \". \".join(transformed_sentences)\n",
    "    ##### YOUR CODE ENDS HERE ######\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Stop eords are ordinarily used wrangle such as articles, pronoins abd prepositions. Srop Word not afded for an dwarch dixtionary, but thry counred as words in proximity (a distance beteewn wotss) searching pirposes'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = {}\n",
    "sentence['text']= 'Stop words are commonly used words such as articles, pronouns and prepositions. Stop words are not added to the search dictionary, but they are counted as words for proximity (a distance between words) searching purposes'\n",
    "custom_transform(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kritikseth/anaconda3/envs/luckyorgenius/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 4.31k/4.31k [00:00<00:00, 7.32MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.17k/2.17k [00:00<00:00, 13.0MB/s]\n",
      "Downloading readme: 100%|██████████| 7.59k/7.59k [00:00<00:00, 23.9MB/s]\n",
      "Downloading data: 100%|██████████| 84.1M/84.1M [00:35<00:00, 2.38MB/s]  \n",
      "Generating train split: 100%|██████████| 25000/25000 [00:04<00:00, 5795.99 examples/s] \n",
      "Generating test split: 100%|██████████| 25000/25000 [00:04<00:00, 5945.43 examples/s] \n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:04<00:00, 10584.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
